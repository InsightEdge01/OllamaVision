Hello everyone! In this video, we’ll be exploring the exciting new release of Llama 3.2 Vision, available on Ollama. This update brings us a collection of multimodal large language models (LLMs), specifically instruction-tuned for image reasoning. The Llama 3.2 Vision models come in both 11B and 90B sizes, designed to handle text + image inputs and generate text-based outputs.

Llama 3.2 Vision is optimized for various tasks like visual recognition, image reasoning, captioning, and answering general questions about images. These models outperforms  many other open-source and closed multimodal models on popular industry benchmarks.

Supported Languages: For text only tasks, English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai are officially supported. Llama 3.2 has been trained on a broader collection of languages than these 8 supported languages. Note for image + text applications, English is the only language supported.

In this video, we’ll experiment with some real-world use cases to see how well these models perform. Toward the end, I’ll show you how to chat with Llama 3.2 Vision using the Ollama Python library within VS Code.

Before we dive in, ensure you’ve installed the latest version of Ollama (v0.4) to follow along. Let’s get started!
